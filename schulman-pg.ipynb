{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-07-07 18:01:50,553] Making new env: Acrobot-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Iteration: \t 0\n",
      "NumTrajs: \t 50\n",
      "NumTimesteps: \t 10000\n",
      "MaxRew: \t -200.0\n",
      "MeanRew: \t -200.0 +- 0.0\n",
      "MeanLen: \t 200.0 +- 0.0\n",
      "-----------------\n",
      "-----------------\n",
      "Iteration: \t 1\n",
      "NumTrajs: \t 50\n",
      "NumTimesteps: \t 10000\n",
      "MaxRew: \t -200.0\n",
      "MeanRew: \t -200.0 +- 0.0\n",
      "MeanLen: \t 200.0 +- 0.0\n",
      "-----------------\n",
      "-----------------\n",
      "Iteration: \t 2\n",
      "NumTrajs: \t 50\n",
      "NumTimesteps: \t 10000\n",
      "MaxRew: \t -200.0\n",
      "MeanRew: \t -200.0 +- 0.0\n",
      "MeanLen: \t 200.0 +- 0.0\n",
      "-----------------\n",
      "-----------------\n",
      "Iteration: \t 3\n",
      "NumTrajs: \t 50\n",
      "NumTimesteps: \t 10000\n",
      "MaxRew: \t -200.0\n",
      "MeanRew: \t -200.0 +- 0.0\n",
      "MeanLen: \t 200.0 +- 0.0\n",
      "-----------------\n",
      "-----------------\n",
      "Iteration: \t 4\n",
      "NumTrajs: \t 50\n",
      "NumTimesteps: \t 10000\n",
      "MaxRew: \t -200.0\n",
      "MeanRew: \t -200.0 +- 0.0\n",
      "MeanLen: \t 200.0 +- 0.0\n",
      "-----------------\n",
      "-----------------\n",
      "Iteration: \t 5\n",
      "NumTrajs: \t 50\n",
      "NumTimesteps: \t 10000\n",
      "MaxRew: \t -200.0\n",
      "MeanRew: \t -200.0 +- 0.0\n",
      "MeanLen: \t 200.0 +- 0.0\n",
      "-----------------\n",
      "-----------------\n",
      "Iteration: \t 6\n",
      "NumTrajs: \t 50\n",
      "NumTimesteps: \t 10000\n",
      "MaxRew: \t -200.0\n",
      "MeanRew: \t -200.0 +- 0.0\n",
      "MeanLen: \t 200.0 +- 0.0\n",
      "-----------------\n",
      "-----------------\n",
      "Iteration: \t 7\n",
      "NumTrajs: \t 50\n",
      "NumTimesteps: \t 10000\n",
      "MaxRew: \t -200.0\n",
      "MeanRew: \t -200.0 +- 0.0\n",
      "MeanLen: \t 200.0 +- 0.0\n",
      "-----------------\n",
      "-----------------\n",
      "Iteration: \t 8\n",
      "NumTrajs: \t 50\n",
      "NumTimesteps: \t 10000\n",
      "MaxRew: \t -200.0\n",
      "MeanRew: \t -200.0 +- 0.0\n",
      "MeanLen: \t 200.0 +- 0.0\n",
      "-----------------\n",
      "-----------------\n",
      "Iteration: \t 9\n",
      "NumTrajs: \t 50\n",
      "NumTimesteps: \t 10000\n",
      "MaxRew: \t -200.0\n",
      "MeanRew: \t -200.0 +- 0.0\n",
      "MeanLen: \t 200.0 +- 0.0\n",
      "-----------------\n",
      "-----------------\n",
      "Iteration: \t 10\n",
      "NumTrajs: \t 50\n",
      "NumTimesteps: \t 10000\n",
      "MaxRew: \t -200.0\n",
      "MeanRew: \t -200.0 +- 0.0\n",
      "MeanLen: \t 200.0 +- 0.0\n",
      "-----------------\n",
      "-----------------\n",
      "Iteration: \t 11\n",
      "NumTrajs: \t 50\n",
      "NumTimesteps: \t 10000\n",
      "MaxRew: \t -200.0\n",
      "MeanRew: \t -200.0 +- 0.0\n",
      "MeanLen: \t 200.0 +- 0.0\n",
      "-----------------\n",
      "-----------------\n",
      "Iteration: \t 12\n",
      "NumTrajs: \t 50\n",
      "NumTimesteps: \t 10000\n",
      "MaxRew: \t -200.0\n",
      "MeanRew: \t -200.0 +- 0.0\n",
      "MeanLen: \t 200.0 +- 0.0\n",
      "-----------------\n",
      "-----------------\n",
      "Iteration: \t 13\n",
      "NumTrajs: \t 50\n",
      "NumTimesteps: \t 10000\n",
      "MaxRew: \t -200.0\n",
      "MeanRew: \t -200.0 +- 0.0\n",
      "MeanLen: \t 200.0 +- 0.0\n",
      "-----------------\n",
      "-----------------\n",
      "Iteration: \t 14\n",
      "NumTrajs: \t 50\n",
      "NumTimesteps: \t 10000\n",
      "MaxRew: \t -200.0\n",
      "MeanRew: \t -200.0 +- 0.0\n",
      "MeanLen: \t 200.0 +- 0.0\n",
      "-----------------\n",
      "-----------------\n",
      "Iteration: \t 15\n",
      "NumTrajs: \t 50\n",
      "NumTimesteps: \t 10000\n",
      "MaxRew: \t -200.0\n",
      "MeanRew: \t -200.0 +- 0.0\n",
      "MeanLen: \t 200.0 +- 0.0\n",
      "-----------------\n",
      "-----------------\n",
      "Iteration: \t 16\n",
      "NumTrajs: \t 50\n",
      "NumTimesteps: \t 10000\n",
      "MaxRew: \t -200.0\n",
      "MeanRew: \t -200.0 +- 0.0\n",
      "MeanLen: \t 200.0 +- 0.0\n",
      "-----------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-60d3a8f0bd7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-60d3a8f0bd7e>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    156\u001b[0m     agent = REINFORCEAgent(env.observation_space, env.action_space, \n\u001b[0;32m    157\u001b[0m         episode_max_length=env.spec.timestep_limit)\n\u001b[1;32m--> 158\u001b[1;33m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-60d3a8f0bd7e>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, env)\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[0mtimesteps_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mtimesteps_total\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"timesteps_per_batch\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m                 \u001b[0mtraj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_traj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"episode_max_length\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m                 \u001b[0mtrajs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m                 \u001b[0mtimesteps_total\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"reward\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-60d3a8f0bd7e>\u001b[0m in \u001b[0;36mget_traj\u001b[1;34m(agent, env, episode_max_length, render)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mrews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode_max_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0mob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-60d3a8f0bd7e>\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, ob)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprob_na\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mob_no\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mob\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcategorical_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m     \"\"\"\n\u001b[1;32m--> 555\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   3496\u001b[0m                        \u001b[1;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3497\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 3498\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 372\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    373\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 636\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    637\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m       \u001b[1;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    706\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 708\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    709\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    713\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 715\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    716\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    695\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m    696\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from tensorflow.contrib.layers import fully_connected as fclayer\n",
    "\n",
    "def discount(x, gamma):\n",
    "    \"\"\"\n",
    "    Given vector x, computes a vector y such that\n",
    "    y[i] = x[i] + gamma * x[i+1] + gamma^2 x[i+2] + ...\n",
    "    \"\"\"\n",
    "    out = np.zeros(len(x), 'float64')\n",
    "    out[-1] = x[-1]\n",
    "    for i in reversed(xrange(len(x)-1)):\n",
    "        out[i] = x[i] + gamma*out[i+1]\n",
    "    assert x.ndim >= 1\n",
    "    # More efficient version:\n",
    "    # scipy.signal.lfilter([1],[1,-gamma],x[::-1], axis=0)[::-1]\n",
    "    return out\n",
    "\n",
    "def categorical_sample(prob_n):\n",
    "    \"\"\"\n",
    "    Sample from categorical distribution,\n",
    "    specified by a vector of class probabilities\n",
    "    \"\"\"\n",
    "    prob_n = np.asarray(prob_n)\n",
    "    csprob_n = np.cumsum(prob_n)\n",
    "    return (csprob_n > np.random.rand()).argmax()\n",
    "\n",
    "\n",
    "def get_traj(agent, env, episode_max_length, render=False):\n",
    "    \"\"\"\n",
    "    Run agent-environment loop for one whole episode (trajectory)\n",
    "    Return dictionary of results\n",
    "    \"\"\"\n",
    "    ob = env.reset()\n",
    "    obs = []\n",
    "    acts = []\n",
    "    rews = []\n",
    "    for _ in xrange(episode_max_length):\n",
    "        a = agent.act(ob)\n",
    "        (ob, rew, done, _) = env.step(a)\n",
    "        obs.append(ob)\n",
    "        acts.append(a)\n",
    "        rews.append(rew)\n",
    "        if done: break\n",
    "        if render: env.render()\n",
    "    return {\"reward\" : np.array(rews),\n",
    "            \"ob\" : np.array(obs),\n",
    "            \"action\" : np.array(acts)\n",
    "            }\n",
    "\n",
    "class REINFORCEAgent(object):\n",
    "\n",
    "    \"\"\"\n",
    "    REINFORCE with baselines\n",
    "    Currently just works for discrete action space\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ob_space, action_space, **usercfg):\n",
    "        \"\"\"\n",
    "        Initialize your agent's parameters\n",
    "        \"\"\"\n",
    "        n0 = ob_space.shape[0]\n",
    "        nA = action_space.n\n",
    "        # Here are all the algorithm parameters. You can modify them by passing in keyword args\n",
    "        self.config = dict(episode_max_length=200, timesteps_per_batch=10000, n_iter=50, \n",
    "            gamma=1.5, stepsize=0.05, nhid=20)\n",
    "        \n",
    "        self.config.update(usercfg)\n",
    "        \n",
    "        self.sess = tf.InteractiveSession()\n",
    "        \n",
    "        self.ob_no = tf.placeholder(tf.float32, shape=(None, n0))\n",
    "        self.a_n = tf.placeholder(tf.int32, shape=(None,))\n",
    "        self.adv_n = tf.placeholder(tf.float32, shape=(None,))\n",
    "        \n",
    "        '''\n",
    "        self.W0 = tf.Variable(tf.random_normal(shape=(n0, self.config['nhid'])) / np.sqrt(n0))\n",
    "        self.b0 = tf.Variable(tf.zeros(shape=(self.config['nhid'])))\n",
    "        \n",
    "        self.W1 = tf.Variable(1e-4 * tf.random_normal(shape=(self.config['nhid'], nA)))\n",
    "        self.b1 = tf.Variable(tf.zeros(shape=(nA)))\n",
    "        \n",
    "        h = tf.tanh(tf.nn.bias_add(tf.matmul(self.ob_no, self.W0), self.b0))\n",
    "        '''\n",
    "        \n",
    "        h1 = fclayer(inputs=self.ob_no, num_outputs=64, activation_fn=tf.nn.tanh, \\\n",
    "                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        h2 = fclayer(inputs=h1, num_outputs=64, activation_fn=tf.nn.tanh, \\\n",
    "                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        self.prob_na = fclayer(inputs=h2, num_outputs=nA, activation_fn=tf.nn.softmax, \\\n",
    "                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        #self.prob_na = tf.nn.softmax(tf.nn.bias_add(tf.matmul(h, self.W1), self.b1))\n",
    "        \n",
    "        a_n_onehot = tf.one_hot(self.a_n, depth=nA)\n",
    "        self.loss = -tf.reduce_mean(tf.mul(tf.log(tf.reduce_sum(self.prob_na * a_n_onehot, reduction_indices=1)), self.adv_n))\n",
    "\n",
    "        \n",
    "        stepsize = self.config['stepsize']\n",
    "        self.train_op = tf.train.RMSPropOptimizer(stepsize).minimize(self.loss)\n",
    "        \n",
    "        self.sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "    def act(self, ob):\n",
    "        prob = self.prob_na.eval(feed_dict={self.ob_no: [ob]})\n",
    "        action = categorical_sample(prob)\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def learn(self, env):\n",
    "        cfg = self.config\n",
    "        \n",
    "        for iteration in xrange(cfg[\"n_iter\"]):\n",
    "            # Collect trajectories until we get timesteps_per_batch total timesteps \n",
    "            trajs = []\n",
    "            timesteps_total = 0\n",
    "            while timesteps_total < cfg[\"timesteps_per_batch\"]:\n",
    "                traj = get_traj(self, env, cfg[\"episode_max_length\"])\n",
    "                trajs.append(traj)\n",
    "                timesteps_total += len(traj[\"reward\"])\n",
    "                \n",
    "            all_ob = np.concatenate([traj[\"ob\"] for traj in trajs])\n",
    "            # Compute discounted sums of rewards\n",
    "            rets = [discount(traj[\"reward\"], cfg[\"gamma\"]) for traj in trajs]\n",
    "            maxlen = max(len(ret) for ret in rets)\n",
    "            padded_rets = [np.concatenate([ret, np.zeros(maxlen-len(ret))]) for ret in rets]\n",
    "            # Compute time-dependent baseline\n",
    "            baseline = np.mean(padded_rets, axis=0)\n",
    "            # Compute advantage function\n",
    "            advs = [ret - baseline[:len(ret)] for ret in rets]\n",
    "            all_action = np.concatenate([traj[\"action\"] for traj in trajs])\n",
    "            all_adv = np.concatenate(advs)\n",
    "            # Do policy gradient update step\n",
    "            \n",
    "            self.sess.run(self.train_op, feed_dict={\n",
    "                    self.ob_no: all_ob, \n",
    "                    self.a_n: all_action,\n",
    "                    self.adv_n: all_adv\n",
    "                })\n",
    "            \n",
    "            eprews = np.array([traj[\"reward\"].sum() for traj in trajs]) # episode total rewards\n",
    "            eplens = np.array([len(traj[\"reward\"]) for traj in trajs]) # episode lengths\n",
    "            # Print stats\n",
    "            print \"-----------------\"\n",
    "            print \"Iteration: \\t %i\"%iteration\n",
    "            print \"NumTrajs: \\t %i\"%len(eprews)\n",
    "            print \"NumTimesteps: \\t %i\"%np.sum(eplens)\n",
    "            print \"MaxRew: \\t %s\"%eprews.max()\n",
    "            print \"MeanRew: \\t %s +- %s\"%(eprews.mean(), eprews.std()/np.sqrt(len(eprews)))\n",
    "            print \"MeanLen: \\t %s +- %s\"%(eplens.mean(), eplens.std()/np.sqrt(len(eplens)))\n",
    "            print \"-----------------\"\n",
    "            get_traj(self, env, cfg[\"episode_max_length\"], render=True)\n",
    "\n",
    "def main():\n",
    "    env = gym.make(\"Acrobot-v0\")\n",
    "    agent = REINFORCEAgent(env.observation_space, env.action_space, \n",
    "        episode_max_length=env.spec.timestep_limit)\n",
    "    agent.learn(env)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
